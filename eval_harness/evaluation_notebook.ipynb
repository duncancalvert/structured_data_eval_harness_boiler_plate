{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Structured Data Evaluation Harness\n",
        "\n",
        "This notebook provides an interactive interface for evaluating generated structured data against ground truth data.\n",
        "\n",
        "## Setup\n",
        "\n",
        "1. Ensure your data files are ready:\n",
        "   - `generated_data.xlsx` or `generated_data.csv` - Your generated data\n",
        "   - `ground_truth.xlsx` or `ground_truth.csv` - Your ground truth data\n",
        "\n",
        "2. Configure the paths and column mappings below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from eval_harness.evaluator import StructuredDataEvaluator\n",
        "from utils.create_test_data import create_test_data_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check Root Dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the project root directory, this assumes the notebook is in eval_harness/ folder\n",
        "current_dir = Path.cwd()\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Check if we're in eval_harness folder, if so go up one level\n",
        "if current_dir.name == 'eval_harness':\n",
        "    PROJECT_ROOT = current_dir.parent\n",
        "else:\n",
        "    # Otherwise assume we're in project root\n",
        "    PROJECT_ROOT = os.getenv(\"PROJECT_ROOT\")\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Current working directory: {Path.cwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables (from project root)\n",
        "env_path = PROJECT_ROOT / \".env\"\n",
        "load_dotenv(env_path)\n",
        "\n",
        "# Configuration - Option to create test data files\n",
        "CREATE_TEST_DATA = True  # Set to True to generate test data files in the data/ folder\n",
        "\n",
        "# Create test data files if requested\n",
        "if CREATE_TEST_DATA:\n",
        "    print(\"Creating test data files...\")\n",
        "    try:\n",
        "        gen_path, gt_path = create_test_data_files(str(PROJECT_ROOT / \"data\"))\n",
        "        print(f\"\\nTest data files created successfully!\")\n",
        "        print(f\"  Generated data: {gen_path}\")\n",
        "        print(f\"  Ground truth: {gt_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating test data files: {e}\")\n",
        "        print(\"Continuing with existing data files...\")\n",
        "\n",
        "# Configuration - Get paths from environment or use defaults\n",
        "# Paths are resolved relative to project root\n",
        "generated_path_env = os.getenv(\"GENERATED_DATA_PATH\", \"data/generated_data.xlsx\")\n",
        "ground_truth_path_env = os.getenv(\"GROUND_TRUTH_PATH\", \"data/ground_truth.xlsx\")\n",
        "\n",
        "# Resolve paths - if absolute, use as-is; if relative, resolve from project root\n",
        "if os.path.isabs(generated_path_env):\n",
        "    GENERATED_DATA_PATH = generated_path_env\n",
        "else:\n",
        "    GENERATED_DATA_PATH = str(PROJECT_ROOT / generated_path_env)\n",
        "\n",
        "if os.path.isabs(ground_truth_path_env):\n",
        "    GROUND_TRUTH_PATH = ground_truth_path_env\n",
        "else:\n",
        "    GROUND_TRUTH_PATH = str(PROJECT_ROOT / ground_truth_path_env)\n",
        "\n",
        "# Verify files exist\n",
        "print(f\"Generated data path: {GENERATED_DATA_PATH}\")\n",
        "print(f\"  File exists: {Path(GENERATED_DATA_PATH).exists()}\")\n",
        "print(f\"Ground truth path: {GROUND_TRUTH_PATH}\")\n",
        "print(f\"  File exists: {Path(GROUND_TRUTH_PATH).exists()}\")\n",
        "\n",
        "# Optional: Column mapping if column names differ\n",
        "# Example: {\"generated_col1\": \"ground_truth_col1\", \"generated_col2\": \"ground_truth_col2\"}\n",
        "COLUMN_MAPPING = None  # Set to dict if needed\n",
        "\n",
        "# Match strategy: \"index\", \"all_pairs\", \"truncate\", or \"key\" (for key-based matching)\n",
        "MATCH_STRATEGY = \"key\"  # Use \"key\" for matching rows by key column value\n",
        "\n",
        "# Key column for matching rows (required when MATCH_STRATEGY=\"key\")\n",
        "# This column should exist in both datasets and contain values that can be matched\n",
        "# (e.g., \"key\", \"name\", \"id\", \"email\", etc.)\n",
        "KEY_COLUMN = \"key\"  # Change to your key column name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the evaluator\n",
        "evaluator = StructuredDataEvaluator(\n",
        "    generated_data_path=GENERATED_DATA_PATH,\n",
        "    ground_truth_path=GROUND_TRUTH_PATH,\n",
        "    column_mapping=COLUMN_MAPPING,\n",
        "    match_strategy=MATCH_STRATEGY,\n",
        "    key_column=KEY_COLUMN if MATCH_STRATEGY == \"key\" else None,\n",
        ")\n",
        "\n",
        "print(f\"Loaded {len(evaluator.generated_df)} rows from generated data\")\n",
        "print(f\"Loaded {len(evaluator.ground_truth_df)} rows from ground truth data\")\n",
        "print(f\"Evaluating {len(evaluator.column_names)} columns: {evaluator.column_names}\")\n",
        "if MATCH_STRATEGY == \"key\" and evaluator.matched_pairs is not None:\n",
        "    matched_count = sum(1 for _, gt_idx, _ in evaluator.matched_pairs if gt_idx is not None)\n",
        "    print(f\"Matched {matched_count} out of {len(evaluator.matched_pairs)} rows using key column '{KEY_COLUMN}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preview Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview generated data\n",
        "print(\"\\n=== Generated Data Preview ===\")\n",
        "display(evaluator.generated_df.head())\n",
        "\n",
        "# Preview ground truth data\n",
        "print(\"\\n=== Ground Truth Data Preview ===\")\n",
        "display(evaluator.ground_truth_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate All Columns\n",
        "\n",
        "This section evaluates all columns using NLP distance and similarity metrics. When using key-based matching (MATCH_STRATEGY=\"key\"), each row in the generated dataset is matched with the corresponding row in the ground truth dataset based on the key column value. The metrics shown below represent the **average** of comparisons between matched rows for each column.\n",
        "\n",
        "Below is a description of each metric and how it's calculated:\n",
        "\n",
        "### Distance and Similarity Metrics\n",
        "\n",
        "**Cosine Similarity** (`cosine_similarity` / `cosine_similarity_tfidf`)\n",
        "- **Range**: 0 to 1 (higher = more similar)\n",
        "- **Description**: Measures the cosine of the angle between two text vectors in a high-dimensional space. Both metrics convert texts into numerical vectors and calculate the cosine of the angle between them, but they differ in how words are weighted:\n",
        "  \n",
        "  - **`cosine_similarity`** (TF only): Uses **Term Frequency (TF)** vectorization. Words are weighted only by how often they appear in each text. Common words like \"the\", \"and\", \"of\" have the same weight as rare, meaningful words. This is simpler and treats all words equally based on frequency.\n",
        "  \n",
        "  - **`cosine_similarity_tfidf`** (Full TF-IDF): Uses **Term Frequency-Inverse Document Frequency (TF-IDF)** vectorization. Words are weighted by both their frequency in the text (TF) and their rarity across the corpus (IDF). Common words are downweighted, while rare, distinctive words are upweighted. This better captures semantic importance and is more robust for comparing texts with different lengths or styles.\n",
        "  \n",
        "  **When to use which**: \n",
        "  - Use `cosine_similarity` (TF) when you want simple word frequency matching without penalizing common words\n",
        "  - Use `cosine_similarity_tfidf` when you want to emphasize distinctive, meaningful words and reduce the impact of common stop words\n",
        "\n",
        "- **Derivation**: `cosine_similarity = (A · B) / (||A|| × ||B||)` where A and B are text vectors (TF or TF-IDF weighted)\n",
        "\n",
        "**Levenshtein Distance** (`levenshtein_distance`)\n",
        "- **Range**: 0 to ∞ (lower = more similar)\n",
        "- **Description**: Also known as edit distance, this measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into another. Lower values indicate more similar texts.\n",
        "- **Derivation**: Calculated using dynamic programming to find the minimum edit operations needed\n",
        "\n",
        "**Edit Distance** (`edit_distance` / `edit_distance_normalized`)\n",
        "- **Range**: \n",
        "  - Raw: 0 to ∞ (lower = more similar)\n",
        "  - Normalized: 0 to 1 (lower = more similar)\n",
        "- **Description**: Same as Levenshtein distance, but the normalized version divides by the maximum length of the two strings to provide a scale-independent measure between 0 and 1.\n",
        "- **Derivation**: `normalized_edit_distance = levenshtein_distance / max(len(text1), len(text2))`\n",
        "\n",
        "**Jaccard Similarity** (`jaccard_similarity_unigram` / `jaccard_similarity_bigram`)\n",
        "- **Range**: 0 to 1 (higher = more similar)\n",
        "- **Description**: Measures similarity as the size of the intersection divided by the size of the union of two sets. \n",
        "  - **Unigram**: Compares sets of individual words\n",
        "  - **Bigram**: Compares sets of character pairs (2-character sequences)\n",
        "- **Derivation**: `jaccard_similarity = |A ∩ B| / |A ∪ B|` where A and B are sets of n-grams\n",
        "\n",
        "### Interpretation Tips\n",
        "\n",
        "- **High cosine similarity** (>0.8): Texts are semantically similar, even if worded differently\n",
        "- **Low Levenshtein/edit distance**: Texts are character-by-character similar (good for detecting typos)\n",
        "- **High Jaccard similarity**: Texts share many common words or character sequences\n",
        "- **Combined metrics**: Using multiple metrics together provides a more robust evaluation, as each captures different aspects of text similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all columns with all metrics\n",
        "results = evaluator.evaluate_all_columns()\n",
        "\n",
        "# Display summary report\n",
        "summary_df = evaluator.get_summary_report(results)\n",
        "display(summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Specific Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate a specific column\n",
        "column_name = evaluator.column_names[0]  # Change to your column name\n",
        "\n",
        "column_results = evaluator.evaluate_column(column_name)\n",
        "\n",
        "print(f\"\\n=== Results for column: {column_name} ===\")\n",
        "print(f\"Number of comparisons: {column_results['num_comparisons']}\")\n",
        "print(\"\\nMetrics:\")\n",
        "for metric_name, metric_stats in column_results.items():\n",
        "    if metric_name not in [\"column_name\", \"num_comparisons\"]:\n",
        "        if isinstance(metric_stats, dict):\n",
        "            print(f\"\\n{metric_name}:\")\n",
        "            for stat_name, stat_value in metric_stats.items():\n",
        "                print(f\"  {stat_name}: {stat_value:.4f}\")\n",
        "        else:\n",
        "            print(f\"{metric_name}: {metric_stats}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Control Which Metrics You Evaluate With"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate with only specific metrics\n",
        "specific_metrics = [\"cosine_similarity\", \"edit_distance\", \"jaccard_similarity\"]\n",
        "\n",
        "results_specific = evaluator.evaluate_all_columns(metrics=specific_metrics)\n",
        "summary_specific = evaluator.get_summary_report(results_specific)\n",
        "\n",
        "display(summary_specific)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get detailed results\n",
        "print(\"\\n=== Detailed Results ===\")\n",
        "print(json.dumps(results, indent=2, default=str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key-Matched Row Comparison Analysis\n",
        "\n",
        "This section shows comparisons between matched rows based on the key column. Each row in the generated dataset is matched with the corresponding row in the ground truth dataset that has the same key value. The metrics shown are calculated only for these matched pairs, and column-level metrics represent the average across all matched pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the column to analyze\n",
        "# Change this to any column name from evaluator.column_names\n",
        "SELECTED_COLUMN = \"name\"  # Options: name, email, address, phone, description\n",
        "\n",
        "# Verify column exists\n",
        "if SELECTED_COLUMN not in evaluator.column_names:\n",
        "    print(f\"Error: Column '{SELECTED_COLUMN}' not found.\")\n",
        "    print(f\"Available columns: {evaluator.column_names}\")\n",
        "else:\n",
        "    print(f\"Selected column: {SELECTED_COLUMN}\")\n",
        "    print(f\"Generated values: {len(evaluator.aligned_generated[SELECTED_COLUMN])}\")\n",
        "    print(f\"Ground truth values: {len(evaluator.aligned_ground_truth[SELECTED_COLUMN])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get matched comparisons for the selected column\n",
        "if evaluator.match_strategy == \"key\" and SELECTED_COLUMN in evaluator.column_names:\n",
        "    matched_df = evaluator.get_matched_comparisons(SELECTED_COLUMN)\n",
        "    \n",
        "    if len(matched_df) > 0:\n",
        "        print(f\"\\n=== Matched Row Comparisons for Column: {SELECTED_COLUMN} ===\")\n",
        "        print(f\"Total matched pairs: {len(matched_df)}\\n\")\n",
        "        display(matched_df)\n",
        "        \n",
        "        # Calculate and display average metrics for this column\n",
        "        print(f\"\\n=== Average Metrics for Column: {SELECTED_COLUMN} ===\")\n",
        "        metric_cols = [col for col in matched_df.columns if col not in \n",
        "                       [\"key\", \"generated_value\", \"ground_truth_value\"]]\n",
        "        \n",
        "        avg_metrics = {}\n",
        "        for metric in metric_cols:\n",
        "            avg_metrics[metric] = matched_df[metric].mean()\n",
        "        \n",
        "        avg_df = pd.DataFrame([avg_metrics]).T\n",
        "        avg_df.columns = [\"average\"]\n",
        "        avg_df = avg_df.sort_values(\"average\", ascending=False)\n",
        "        display(avg_df)\n",
        "        \n",
        "        print(f\"\\nNote: These averages represent the mean similarity/distance metrics\")\n",
        "        print(f\"across all {len(matched_df)} matched rows (based on key column '{evaluator.key_column}').\")\n",
        "    else:\n",
        "        print(f\"No matched pairs found for column '{SELECTED_COLUMN}'.\")\n",
        "        print(\"This may indicate that no rows in the generated data matched with ground truth rows.\")\n",
        "else:\n",
        "    print(\"Please ensure key-based matching is enabled and column is valid.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export summary to CSV in the data folder\n",
        "output_path = PROJECT_ROOT / \"data\" / \"evaluation_results.csv\"\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure data folder exists\n",
        "summary_df.to_csv(output_path, index=False)\n",
        "print(f\"Results exported to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
